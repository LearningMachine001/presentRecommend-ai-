import re
from collections import defaultdict
from keybert import KeyBERT
from konlpy.tag import Okt
from kobert_tokenizer import KoBERTTokenizer
from transformers import BertForSequenceClassification, AutoTokenizer
from sentence_transformers import SentenceTransformer, util
import pandas as pd
import torch
import torch.nn as nn
from transformers import BertModel

# ê´€ì‹¬ ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ
model_name = "skt/kobert-base-v1"
tokenizer = KoBERTTokenizer.from_pretrained(model_name)
interest_model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)
interest_model.load_state_dict(torch.load("./kobert_importance.pth", map_location="cpu"))
interest_model.eval()

# í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì„ë² ë”© ëª¨ë¸
kw_model = KeyBERT(model="distiluse-base-multilingual-cased-v1")
embedding_model = SentenceTransformer("jhgan/ko-sroberta-multitask")
okt = Okt()

# ì£¼ì œ ë¶„ë¥˜ ëª¨ë¸ ì •ì˜
class KoBertExtendedModel(nn.Module):
    def __init__(self, model_name="skt/kobert-base-v1", num_subjects=20):
        super().__init__()
        self.bert = BertModel.from_pretrained(model_name)
        self.score_head = nn.Linear(768, 1)
        self.awkward_head = nn.Linear(768, 2)
        self.subject_head = nn.Linear(768, num_subjects)

    def forward(self, input_ids, attention_mask):
        pooled_output = self.bert(input_ids=input_ids, attention_mask=attention_mask).pooler_output
        score = self.score_head(pooled_output)
        awkward = self.awkward_head(pooled_output)
        subject = self.subject_head(pooled_output)
        return score, awkward, subject

# ì£¼ì œ id â†’ ì´ë¦„, ëŒ€ë¶„ë¥˜ ë§¤í•‘
subject_id2name = {
    0: "ë¯¸ìš©", 1: "ìŠ¤í¬ì¸ /ë ˆì €", 2: "êµìœ¡", 3: "ê°€ì¡±", 5: "ì˜í™”/ë§Œí™”",
    6: "êµí†µ", 7: "ì—¬í–‰", 8: "íšŒì‚¬/ì•„ë¥´ë°”ì´íŠ¸", 9: "ê±´ê°•", 10: "ì—°ì• /ê²°í˜¼",
    11: "ê²Œì„", 12: "ê³„ì ˆ/ë‚ ì”¨", 13: "ë°©ì†¡/ì—°ì˜ˆ", 14: "ì‚¬íšŒì´ìŠˆ",
    15: "ì£¼ê±°ì™€ ìƒí™œ", 16: "ë°˜ë ¤ë™ë¬¼", 17: "êµ°ëŒ€", 18: "ì‹ìŒë£Œ"
}

subject_to_main_category = {
    0: "ë·°í‹°", 1: "ë ˆì €/ìŠ¤í¬ì¸ ", 2: "ë¦¬ë¹™/ë„ì„œ", 3: "ë””ì§€í„¸/ê°€ì „", 5: "íŒ¨ì…˜",
    6: "ë””ì§€í„¸/ê°€ì „", 7: "ë ˆì €/ìŠ¤í¬ì¸ ", 8: "ë¦¬ë¹™/ë„ì„œ", 9: "ì‹í’ˆ", 10: "íŒ¨ì…˜",
    11: "ë””ì§€í„¸/ê°€ì „", 12: "ì‹í’ˆ", 13: "íŒ¨ì…˜", 14: "ë¦¬ë¹™/ë„ì„œ",
    15: "ë¦¬ë¹™/ë„ì„œ", 16: "ìœ ì•„ë™/ë°˜ë ¤", 17: "ì‹í’ˆ", 18: "ì‹í’ˆ"
}

# ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ
topic_tokenizer = AutoTokenizer.from_pretrained("skt/kobert-base-v1", use_fast=False)
topic_model = KoBertExtendedModel()
topic_model.load_state_dict(torch.load("kobert_extended_with_subject.pth", map_location="cpu"), strict=False)
topic_model.eval()

# ë¶ˆìš©ì–´ ë¡œë“œ
with open("stopwords-ko.txt", encoding="utf-8") as f:
    stopwords = set(line.strip() for line in f if line.strip())

def classify_interest(sentence):
    inputs = tokenizer(sentence, return_tensors="pt", padding=True, truncation=True, max_length=128)
    with torch.no_grad():
        outputs = interest_model(**inputs)
        probs = torch.softmax(outputs.logits, dim=1)
        return int(torch.argmax(probs, dim=1))

def classify_topic_and_score(sentence):
    inputs = topic_tokenizer(
        sentence,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=512
    )
    input_ids = inputs["input_ids"]
    attention_mask = inputs["attention_mask"]

    with torch.no_grad():
        score, _, subject_logits = topic_model(input_ids, attention_mask)

        # ì¹œë°€ë„ ì ìˆ˜ (0~5 ìŠ¤ì¼€ì¼)
        score_value = torch.sigmoid(score).item() * 5

        # ì£¼ì œ ë¶„ë¥˜
        subject_id = torch.argmax(subject_logits, dim=1).item()
        subject_name = subject_id2name.get(subject_id, "ì•Œ ìˆ˜ ì—†ìŒ")
        main_category = subject_to_main_category.get(subject_id, "ì—†ìŒ")

    return subject_name, main_category, round(score_value, 2)

def extract_kakao_dialogues(path):
    with open(path, encoding="utf-8") as f:
        lines = f.readlines()
    data_by_date = defaultdict(list)
    for line in lines:
        match = re.search(r"(\d{4})ë…„ (\d{1,2})ì›” (\d{1,2})ì¼", line)
        if match:
            y, m, d = match.groups()
            current_date = f"{int(y):04d}-{int(m):02d}-{int(d):02d}"
        elif re.search(r"[ì˜¤ì „|ì˜¤í›„]+\s*\d{1,2}:\d{2},\s*[^:]+:", line):
            msg = re.sub(r"^\d{4}\. \d{1,2}\. \d{1,2}\. [ì˜¤ì „|ì˜¤í›„]+\s*\d{1,2}:\d{2},\s*[^:]+:\s*", "", line).strip()
            if len(msg) > 0:
                data_by_date[current_date].append(msg)
    return data_by_date

def is_valid_conversation(msg):
    return bool(re.search(r"[ê°€-í£]", msg)) and not re.search(r"https?://|ì´\s*ê¸ˆì•¡", msg)

def extract_interest_weighted_keywords(sentences):
    keyword_scores = defaultdict(float)
    for sentence in sentences:
        label = classify_interest(sentence)
        nouns = {n for n in okt.nouns(sentence) if n not in stopwords and len(n) > 1}
        keywords = kw_model.extract_keywords(sentence, keyphrase_ngram_range=(1, 2), stop_words=None, top_n=5)

        for kw, score in keywords:
            tokens = kw.split()
            if all(token in nouns for token in tokens):
                multiplier = 2.5 if len(tokens) > 1 else 2.0
                final_score = score * (multiplier if label == 1 else 0.5)
                keyword_scores[kw] += final_score

        for noun in nouns:
            add_score = 0.3 if label == 1 else 0.1
            keyword_scores[noun] += add_score

    filtered_keywords = [(kw, sc) for kw, sc in keyword_scores.items()
                      if all(not re.search(r"(ë‹¤|ì–´|ì§€|ìŒ)$", token) for token in kw.split())]
    return sorted(filtered_keywords, key=lambda x: x[1], reverse=True)

# ëŒ€ë¶„ë¥˜ â†’ ëŒ€ì‘ íŒŒì¼ ê²½ë¡œ ë§¤í•‘
category_to_file = {
    "ë·°í‹°": "category_files/beauty.csv",
    "ë ˆì €/ìŠ¤í¬ì¸ ": "category_files/sport.csv",
    "ë¦¬ë¹™/ë„ì„œ": "category_files/living.csv",
    "ë””ì§€í„¸/ê°€ì „": "category_files/digital.csv",
    "íŒ¨ì…˜": "category_files/fashion.csv",
    "ì‹í’ˆ": "category_files/food.csv",
    "ìœ ì•„ë™/ë°˜ë ¤": "category_files/baby.csv"
}

product_list = []
product_embeddings = []

product_embeddings = [
    {
        "name": p["name"],
        "embedding": embedding_model.encode(p["keywords"], convert_to_tensor=True),
        "category": p["category"]
    }
    for p in product_list
]

def recommend_products_from_keywords(sorted_keywords, allowed_category=None):
    global product_list, product_embeddings

    # ì¹´í…Œê³ ë¦¬ ëŒ€ì‘ íŒŒì¼ ë¶„ê¸°
    if allowed_category:
        csv_path = category_to_file.get(allowed_category)
        if not csv_path:
            print(f"[!] '{allowed_category}' ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•˜ëŠ” CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
            return []
        product_df = pd.read_csv(csv_path)
        product_list = [
            {"name": row["ìƒí’ˆëª…"], "keywords": row["keywords"], "category": row["ëŒ€ë¶„ë¥˜"]}
            for _, row in product_df.iterrows()
            if pd.notna(row["keywords"]) and pd.notna(row["ìƒí’ˆëª…"])
        ]
        product_embeddings = [
            {
                "name": p["name"],
                "embedding": embedding_model.encode(p["keywords"], convert_to_tensor=True),
                "category": p["category"]
            }
            for p in product_list
        ]
    query = " ".join([kw for kw, _ in sorted_keywords[:5]])
    query_embedding = embedding_model.encode(query, convert_to_tensor=True)
    scores = []
    for prod in product_embeddings:
        if allowed_category and prod["category"] != allowed_category:
            continue
        score = util.cos_sim(query_embedding, prod["embedding"]).item()
        scores.append((prod["name"], score))
    return sorted(scores, key=lambda x: x[1], reverse=True)

file_path = "chat_exam.txt"
data_by_date = extract_kakao_dialogues(file_path)

for date, messages in sorted(data_by_date.items()):
    filtered_msgs = [msg for msg in messages if is_valid_conversation(msg)]
    if len(filtered_msgs) == 0:
        continue

    # ë‚ ì§œë³„ ì „ì²´ ëŒ€í™”ë¬¸ì„ ì—°ê²°í•´ ì£¼ì œ ì¶”ë¡ 
    full_text = " ".join(filtered_msgs)
    subject_name, main_category, intimacy_score = classify_topic_and_score(full_text)

    keywords = extract_interest_weighted_keywords(filtered_msgs)
    print(f"\nğŸ“… {date} ì£¼ì œ: {subject_name}, ëŒ€ë¶„ë¥˜: {main_category}, ì¹œë°€ë„ ì ìˆ˜: {intimacy_score}")
    for kw, score in keywords[:5]:
        print(f"- {kw}: {score:.2f}")

    print("\nğŸ ì¶”ì²œ ì„ ë¬¼ TOP 10:")
    for name, score in recommend_products_from_keywords(keywords, allowed_category=main_category)[:5]:
        print(f"- {name} ({score:.2f})")